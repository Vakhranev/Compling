{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM8gZmJvYrl1WyEK089W4BV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/Compling/blob/master/Diplom4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fjsy7qUP5Dr"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V15AjyLhP-XA"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flVb34q-qV-F"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjvEs1oPqcfy",
        "outputId": "0a16d28f-7c3f-4edd-99c4-f5c8e23d382c"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFCc5ghEzQVA",
        "outputId": "1aad05a4-0f05-44d9-905e-aea4736d5e33"
      },
      "source": [
        "!pip install pynput"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pynput\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f7/cb6e7edb8bb46bb38ac8c4f8065e6e3abaaef3436ecd7fe139ca5ba93c3c/pynput-1.7.3-py2.py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pynput) (1.15.0)\n",
            "Collecting python-xlib>=0.17; \"linux\" in sys_platform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/c3/45ecfd3e6a541bb4b383fc320a32762703cfe28763c131d71f4183ace819/python_xlib-0.30-py2.py3-none-any.whl (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 16.8MB/s \n",
            "\u001b[?25hCollecting evdev>=1.3; \"linux\" in sys_platform\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/ec/bb298d36ed67abd94293253e3e52bdf16732153b887bf08b8d6f269eacef/evdev-1.4.0.tar.gz\n",
            "Building wheels for collected packages: evdev\n",
            "  Building wheel for evdev (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evdev: filename=evdev-1.4.0-cp37-cp37m-linux_x86_64.whl size=97430 sha256=9341003ffc66bdf98bc999245398bff52b9a7301dfb5cfa69e04909d253e54b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/35/1d/38b2de1421ede48ebf0107faea56163d07059110b116a794a3\n",
            "Successfully built evdev\n",
            "Installing collected packages: python-xlib, evdev, pynput\n",
            "Successfully installed evdev-1.4.0 pynput-1.7.3 python-xlib-0.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I4pj-0Hm_JL",
        "outputId": "8b5c7f3d-f530-4dee-9aa3-c1c4cd235fa7"
      },
      "source": [
        "!pip install patool"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 5.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kQuffDZnHau"
      },
      "source": [
        "import patoolib"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpYKO8Hfq8Py"
      },
      "source": [
        "archive = '/content/drive/My Drive/lenta-ru-news.rar'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKGXHhrmnLEn",
        "outputId": "d2d7d9a2-5068-42dd-c56c-3c97dbf229d3"
      },
      "source": [
        "patoolib.extract_archive(archive)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "patool: Extracting /content/drive/My Drive/lenta-ru-news.rar ...\n",
            "patool: running /usr/bin/unrar x -- \"/content/drive/My Drive/lenta-ru-news.rar\"\n",
            "patool:     with cwd='./Unpack_ohyzoe_z'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQfMeLOuSjVS"
      },
      "source": [
        "corpora = pd.read_csv('lenta-ru-news.csv',engine='python', error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4L8CwAyTlAl"
      },
      "source": [
        "corpora.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQurJOoTv5H"
      },
      "source": [
        "corpora['text'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVcrViOPWcU9"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhTN8bdSUk29"
      },
      "source": [
        "new_corpora = []\n",
        "for text in corpora['text']:\n",
        "  split_regex = re.compile(r'[.|!|?|…]')\n",
        "  sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(str(text))])\n",
        "  for sentence in sentences:\n",
        "    new_corpora.append(sentence)\n",
        "print(len(new_corpora))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUQbnlBEYEuB"
      },
      "source": [
        "new_corpora[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZAeU9qnqZOC"
      },
      "source": [
        "!pip install ufal.udpipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x70ogR4Qxn7K"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh4aDZpixwka"
      },
      "source": [
        "import wget\n",
        "import sys\n",
        "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
        "modelfile = wget.download(udpipe_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYJ2SPs5yQ_N"
      },
      "source": [
        "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
        "   entities = {'PROPN'}\n",
        "   named = False  # переменная для запоминания того, что нам встретилось имя собственное\n",
        "   memory = []\n",
        "   mem_case = None\n",
        "   mem_number = None\n",
        "   tagged_propn = []\n",
        "# обрабатываем текст, получаем результат в формате conllu:\n",
        "   processed = pipeline.process(text)\n",
        "# пропускаем строки со служебной информацией:\n",
        "   content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
        "# извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
        "   tagged = [w.split('\\t') for w in content if w]\n",
        "   for t in tagged:\n",
        "       if len(t) != 10: # если список короткий — строчка не содержит разбора, пропускаем\n",
        "           continue\n",
        "       (word_id,token,lemma,pos,xpos,feats,head,deprel,deps,misc) = t \n",
        "       if not lemma or not token: # если слово пустое — пропускаем\n",
        "           continue\n",
        "       if pos in entities: # здесь отдельно обрабатываем имена собственные — они требуют особого обращения\n",
        "           if '|' not in feats:\n",
        "               tagged_propn.append('%s_%s_%s_%s_%s_%s' % (word_id, token, lemma, pos, head, deprel))\n",
        "               continue\n",
        "           morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
        "           if 'Case' not in morph or 'Number' not in morph:\n",
        "               tagged_propn.append('%s_%s_%s_%s_%s_%s' % (word_id, token, lemma, pos, head, deprel))\n",
        "               continue\n",
        "           if not named:\n",
        "               named = True\n",
        "               mem_case = morph['Case']\n",
        "               mem_number = morph['Number']\n",
        "           if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
        "               memory.append(lemma)\n",
        "               if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
        "                   named = False\n",
        "                   past_lemma = '::'.join(memory)\n",
        "                   memory = []\n",
        "                   tagged_propn.append(past_lemma + '_PROPN ')\n",
        "           else:\n",
        "               named = False\n",
        "               past_lemma = '::'.join(memory)\n",
        "               memory = []\n",
        "               tagged_propn.append(past_lemma + '_PROPN ')\n",
        "               tagged_propn.append('%s_%s_%s_%s_%s_%s' % (word_id, token, lemma, pos, head, deprel))\n",
        "       else:\n",
        "           if not named:\n",
        "               tagged_propn.append('%s_%s_%s_%s_%s_%s' % (word_id, token, lemma, pos, head, deprel))\n",
        "           else:\n",
        "               named = False\n",
        "               past_lemma = '::'.join(memory)\n",
        "               memory = []\n",
        "               tagged_propn.append(past_lemma + '_PROPN ')\n",
        "               tagged_propn.append('%s_%s_%s_%s_%s_%s' % (word_id, token, lemma, pos, head, deprel))\n",
        "   if not keep_punct: # обрабатываем случай, когда пользователь попросил не сохранять пунктуацию (по умолчанию она сохраняется)\n",
        "       tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
        "   if not keep_pos:\n",
        "       tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
        "   return tagged_propn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhBa9_Xozs02"
      },
      "source": [
        "from ufal.udpipe import Model, Pipeline\n",
        "import os\n",
        "import re\n",
        "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
        "   udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
        "   udpipe_filename = udpipe_model_url.split('/')[-1]\n",
        "   if not os.path.isfile(modelfile):\n",
        "       print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
        "       wget.download(udpipe_model_url)\n",
        "   #print('\\nLoading the model...', file=sys.stderr)\n",
        "   model = Model.load(modelfile)\n",
        "   process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
        "   #print('Processing input...', file=sys.stderr)\n",
        "   lines = text.split('\\n')\n",
        "   tagged = []\n",
        "   for line in lines:\n",
        "# line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
        "       output = process(process_pipeline, text=line)\n",
        "       tagged_line = ' '.join(output)\n",
        "       tagged.append(tagged_line)\n",
        "   return '\\n'.join(tagged)\n",
        "text = new_corpora[100000:120000]\n",
        "for sentence in text:\n",
        "   processed_text = tag_ud(text=sentence, modelfile=modelfile)\n",
        "   #print(processed_text[:350])\n",
        "   with open('my_text.txt', 'a', encoding='utf-8') as out:\n",
        "       out.write(processed_text + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGbS5w8jaE_r"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di7f3w9ZatJw"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQO9mUQhkSre"
      },
      "source": [
        "new_corpora = open(\"my_text.txt\", encoding=\"UTF-8\", errors='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szLXqVvzaIg7"
      },
      "source": [
        "tokens=[]\n",
        "for sentence in new_corpora:\n",
        "  tokens.append(nltk.word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0I63eV-eAeS"
      },
      "source": [
        "tokens[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ml532Zfg9I1"
      },
      "source": [
        "law_corpora=[]\n",
        "for sentence in tokens:\n",
        "  for token in sentence:\n",
        "    if \"_суд_\" in token:\n",
        "      if sentence not in law_corpora:\n",
        "        law_corpora.append(sentence)\n",
        "    elif \"_судья_\" in token:\n",
        "      if sentence not in law_corpora:\n",
        "        law_corpora.append(sentence)\n",
        "    elif \"_адвокат_\" in token:\n",
        "      if sentence not in law_corpora:\n",
        "        law_corpora.append(sentence)\n",
        "    elif \"_прокурор_\" in token:\n",
        "      if sentence not in law_corpora:\n",
        "        law_corpora.append(sentence)\n",
        "    else:\n",
        "      pass\n",
        "print(len(law_corpora))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Rh5PVLh4JQ"
      },
      "source": [
        "law_corpora[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3LdUA18kKI9"
      },
      "source": [
        "with open(\"law_corpora.txt\", \"w\") as file:\n",
        "  print(law_corpora, file=file, sep=\"\\n\")\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7J2KSMJJ2Lo"
      },
      "source": [
        "count=0\n",
        "for sentence in tokens:\n",
        "  count += len(sentence)\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVDios2gKPKM"
      },
      "source": [
        "count=0\n",
        "for sentence in law_corpora:\n",
        "  count += len(sentence)\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEd9vRdqlG2T"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}