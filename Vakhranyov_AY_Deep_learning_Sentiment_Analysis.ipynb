{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vakhranyov_AY_Deep_learning_Sentiment_Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWmHlfDylOqAjOzi0xkFjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/Compling/blob/master/Vakhranyov_AY_Deep_learning_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTtt8gqJt9Mx"
      },
      "source": [
        "В этом проекте я постарался с помощью предобученного берта сделать предсказания для текста на сентимент-анализ. Я взял датасет Sentiment Analysis Dataset.csv он есть здесь (https://github.com/vineetdhanawat/twitter-sentiment-analysis/blob/master/datasets/Sentiment%20Analysis%20Dataset.csv). Пояснений к нему, к сожалению, нет, поэтому точно сказать не могу, но, судя по всему, представленные в нём разбиты следующим образом: целевой класс (представлены единицей) — твиты с положительно-окрашенным мнением, всё остальное — представлено нулями. Помимо того, что я попробовал замерять лоссы на эпохах, также я использовал ф-меру для оценки модели.\r\n",
        "\r\n",
        "В последнее время применение глубокого обучения для решения проблемы сентимент-анализа стало популярной темой исследований. Существуют различные архитектуры глубокого обучения и технологии, которые применяют для подобного рода анализа: эмбеддинги, автоэнкодеры, CNN, RNN, LSTM, применение аттеншн-механизма в RNN, MemNN, RecNN. Многие из этих методов глубокого обучения показали отличные результаты для различных задач сентимент-анализа. Если верить научным работам по теме, которые я просмотрел, то с развитием исследований и приложений глубокого обучения в ближайшем будущем появятся более интересные исследования применения глубокого обучения для сентимент-анализа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ToXGJY54fuh",
        "outputId": "bb76f610-2293-4d95-8002-b185333dc58b"
      },
      "source": [
        "!pip install transformers\r\n",
        "import pandas as pd\r\n",
        "from transformers import AutoTokenizer\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "data = pd.read_csv(\"Sentiment Analysis Dataset.csv\", header= None)\r\n",
        "max_len = 512"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taWr1zOMUC5P"
      },
      "source": [
        "data = data[:500]\r\n",
        "data.columns = [\"id\", \"sentiment\", \"text\"]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-6jLXWe0nK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "train, test = train_test_split(data, test_size=0.1, random_state=42)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaf9dNDBDrHz"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "class dataset(Dataset):\r\n",
        "    def __init__(self, texts, targets, tokenizer, max_len):\r\n",
        "        self.texts = texts\r\n",
        "        self.targets = targets\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_len = max_len\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.texts)\r\n",
        "\r\n",
        "    def __getitem__(self, item):   \r\n",
        "        text = str(self.texts[item])\r\n",
        "        target = self.targets[item]\r\n",
        "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True,max_length=self.max_len, return_token_type_ids=False,padding='max_length', return_attention_mask=True, return_tensors='pt', truncation = True)\r\n",
        "        return {'text': text, 'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'targets': torch.tensor(target, dtype=torch.long)}"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVwydfykWqxL",
        "outputId": "38458433-cb5c-4d66-d882-13d76d163c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.text.head"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of 72       I think I may be too friendly...lol... o wel...\n",
              "182     @adbert: &quot;#Video [Woody Woodpecker вЂ“ T...\n",
              "131      you'll always be my one and only...  [12*23*...\n",
              "410         A dog riding the bicycle http://bit.ly/gvMzD\n",
              "193     @dandelionas is making fettucini and garlic b...\n",
              "                             ...                        \n",
              "106      really wanted Safina to pull out a win &amp;...\n",
              "270       lol all these #robotpickuplines are hilarious \n",
              "348                            what the fucccckkkkkkkkkk\n",
              "435      I dont want my sister to leave me for the wh...\n",
              "102                                    not a cool night.\n",
              "Name: text, Length: 450, dtype: object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ojHfrATG3iK",
        "outputId": "bf4d1c7c-01b4-4fc8-914f-4274760879c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import numpy as np\r\n",
        "def Dataloader(data, tokenizer, max_len, batch_size):\r\n",
        "    ds = dataset(data.text.to_numpy(), data.sentiment.to_numpy(), tokenizer=tokenizer, max_len=max_len)\r\n",
        "    return Dataloader(ds,tokenizer=tokenizer,max_len=max_len,batch_size=batch_size)\r\n",
        "\r\n",
        "batch_size = 8\r\n",
        "loader = Dataloader(train, tokenizer, max_len, batch_size)\r\n",
        "test_loader = Dataloader(test, tokenizer, max_len, batch_size)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-3019b3d8c9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-3019b3d8c9e4>\u001b[0m in \u001b[0;36mDataloader\u001b[0;34m(data, tokenizer, max_len, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-3019b3d8c9e4>\u001b[0m in \u001b[0;36mDataloader\u001b[0;34m(data, tokenizer, max_len, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dataset' object has no attribute 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIhtdUDWOSx7"
      },
      "source": [
        "model = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUvNCY6cRJ29"
      },
      "source": [
        "class Classifier(torch.nn.Module):\r\n",
        "    def __init__(self, bert, hidden_size=768, output_size=2):\r\n",
        "        super().__init__()\r\n",
        "        self.bert = bert\r\n",
        "        self.classifier = torch.nn.Linear(hidden_size, output_size)\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        pred = self.bert(x, attention_mask=(x!=0))\r\n",
        "        return self.classifier(pred.pooler_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Uk7zLoSnEY"
      },
      "source": [
        "epochs = 6\r\n",
        "for param in model.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "loss = torch.nn.CrossEntropyLoss()\r\n",
        "classifier = Classifier(model)\r\n",
        "optimizer = torch.optim.Adam(classifier.classifier.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwdTzjE8Wp2-"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import numpy as np\r\n",
        "train_losses = list()\r\n",
        "train_loss_per_epoch = list()\r\n",
        "test_loss_per_epoch = list()\r\n",
        "train_f1_scores_per_epoch = list()\r\n",
        "test_f1_scores_per_epoch = list()\r\n",
        "best_test_metric_value = float('inf')\r\n",
        "for epoch in range(epochs):\r\n",
        "    train_epoch_losses = list()\r\n",
        "    test_epoch_losses = list()\r\n",
        "    train_predictions = list()\r\n",
        "    train_targets = list()\r\n",
        "    test_predictions = list()\r\n",
        "    test_targets = list()\r\n",
        "    print(f'Epoch: {epoch}')\r\n",
        "    classifier.train()\r\n",
        "    for x, y in tqdm(loader):\r\n",
        "        x = x.long()\r\n",
        "        pred = classifier(x)\r\n",
        "        train_loss = loss(pred, y)\r\n",
        "        train_loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        train_losses.append(train_loss.item())\r\n",
        "        train_epoch_losses.append(train_loss.item())\r\n",
        "        train_predictions.extend(pred.argmax(dim=1).tolist())\r\n",
        "        train_targets.extend(y.tolist())\r\n",
        "    classifier.eval()\r\n",
        "    for x, y in tqdm(test_loader):\r\n",
        "        x = x.long()\r\n",
        "        with torch.no_grad():\r\n",
        "            pred = classifier(x)\r\n",
        "            test_loss = loss(pred, y)\r\n",
        "        test_epoch_losses.append(test_loss.item())\r\n",
        "        test_predictions.extend(pred.argmax(dim=1).tolist())\r\n",
        "        test_targets.extend(y.tolist())\r\n",
        "    train_loss_per_epoch.append(np.mean(train_epoch_losses))\r\n",
        "    test_loss_per_epoch.append(np.mean(test_epoch_losses))\r\n",
        "    train_f1_scores_per_epoch.append(f1_score(train_targets, train_predictions))\r\n",
        "    test_f1_scores_per_epoch.append(f1_score(test_targets, test_predictions))\r\n",
        "    print(test_f1_scores_per_epoch)\r\n",
        "    f1 = f1_score(y, pred.argmax(dim=1))\r\n",
        "    print(f'Epoch {epoch}')\r\n",
        "    print(f'Loss: train {train_loss_per_epoch[-1]:.2f} | test {test_loss_per_epoch[-1]:.2f}')\r\n",
        "    print(f'F1 Score: train {train_f1_scores_per_epoch[-1]:.2f} | test {test_f1_scores_per_epoch[-1]:.2f}')\r\n",
        "    #if best_test_metric_value > test_f1_scores_per_epoch[-1]:\r\n",
        "    #  best_test_metric_value = test_f1_scores_per_epoch[-1]\r\n",
        "    #else:\r\n",
        "    #  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahd-meNFHfYt"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "plt.figure(figsize=(14, 12))\r\n",
        "plt.plot(train_loss_per_epoch)\r\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8MIrRg4ppTq"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "plt.figure(figsize=(14, 12))\r\n",
        "plt.plot(test_loss_per_epoch)\r\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVvWpwVCqFKk"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "plt.figure(figsize=(14, 12))\r\n",
        "plt.plot(train_f1_scores_per_epoch)\r\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P47ebUkLqkes"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "plt.figure(figsize=(14, 12))\r\n",
        "plt.plot(test_f1_scores_per_epoch)\r\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lomEqQRvqxlS"
      },
      "source": [
        "print('Лучшая метрика на test:')\r\n",
        "print('F1 test: ', max(test_f1_scores_per_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AAKvuc60v_k"
      },
      "source": [
        "Я выдающимися знаниями особо никогда не блистал, поэтому для меня даже достаточно простая задача (если сравнивать с другими вариантами проектов) давалась непросто. Наверное, главная проблема, с которой я столкнулся — это подсчёт ф-меры на большом количестве данных на тесте. На сотнях и десятках тысяч я упорно получал нули после каждой эпохи. В причнах этой проблемы, к сожалению, мне разобраться не удалось. Я пробовал увеличивать количество эпох обучения, но результат не менялся. Уменьшать learning rate я не стал, потому что по умолчанию он и так стоит небольшой, вряд ли это помогло бы с решением проблемы. В результате, относительного успеха удалось добиться, снизив объём данных до 2000.\r\n",
        "Но здесь, само собой, были свои проблемы.\r\n",
        "Например, ф-мера на трейне была довольно низкая, а лоссы на трейне вообще вели себя довольно хаотично. Также, как видно на графиках, на 6 (последней) эпохе резко портятся метрики, что, вероятнее всего, свидетельствует о переобучении модели. Думаю, что на таком количестве данных оптимально было бы остановиться на 5 эпохах.\r\n",
        "Тем не менее, мне удалось получить ф-меру равную 0.77 на тесте, что является достаточно хорошим результатом."
      ]
    }
  ]
}